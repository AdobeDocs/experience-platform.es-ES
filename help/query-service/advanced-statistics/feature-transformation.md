---
title: T茅cnicas de transformaci贸n de funciones
description: Obtenga informaci贸n acerca de las t茅cnicas de preprocesamiento esenciales, como la transformaci贸n de datos, la codificaci贸n y la escala de caracter铆sticas, que preparan los datos para la formaci贸n del modelo estad铆stico. Cubre la importancia de gestionar los valores que faltan y convertir los datos categ贸ricos para mejorar el rendimiento y la precisi贸n del modelo.
role: Developer
exl-id: ed7fa9b7-f74e-481b-afba-8690ce50c777
source-git-commit: e7bc30c153f67c59e9c04e8c8df60394f48871d0
workflow-type: tm+mt
source-wordcount: '3450'
ht-degree: 8%

---

# T茅cnicas de transformaci贸n de funciones

Las transformaciones son pasos de preprocesamiento cruciales que convierten o escalan los datos en un formato adecuado para la formaci贸n y el an谩lisis de modelos, lo que garantiza un rendimiento y una precisi贸n 贸ptimos. Este documento sirve como recurso de sintaxis complementario, que proporciona detalles exhaustivos sobre las t茅cnicas clave de transformaci贸n de funciones para el preprocesamiento de datos.

Los modelos de aprendizaje autom谩tico no pueden procesar directamente valores de cadena o valores nulos, por lo que el preprocesamiento de datos es esencial. En esta gu铆a se explica c贸mo utilizar varias transformaciones para imputar valores que faltan, convertir datos categ贸ricos en formatos num茅ricos y aplicar t茅cnicas de escalado de funciones como codificaci贸n y vectorizaci贸n en un solo paso. Estos m茅todos permiten a los modelos interpretar y aprender eficazmente de los datos, mejorando finalmente su rendimiento.

## Transformaci贸n autom谩tica de funciones {#automatic-transformations}

Si elige omitir la cl谩usula `TRANSFORM` en el comando `CREATE MODEL`, la transformaci贸n de caracter铆sticas se produce autom谩ticamente. El preprocesamiento autom谩tico de datos incluye el reemplazo nulo y las transformaciones de funciones est谩ndar (seg煤n el tipo de datos). Las columnas num茅ricas y de texto se imputan autom谩ticamente y, a continuaci贸n, se llevan a cabo transformaciones de funciones para garantizar que los datos tengan un formato adecuado para la formaci贸n del modelo de aprendizaje autom谩tico. Este proceso incluye la imputaci贸n de datos que faltan y transformaciones categ贸ricas, num茅ricas y booleanas.

>[!IMPORTANT]
>
>La transformaci贸n de funciones utilizada en el momento del entrenamiento tambi茅n se utilizar谩 para la transformaci贸n de funciones en el momento de la predicci贸n y evaluaci贸n.

En las tablas siguientes se explica c贸mo se controlan los distintos tipos de datos cuando se omite la cl谩usula `TRANSFORM` durante el comando `CREATE MODEL`.

### Reemplazo nulo {#automatic-null-replacement}

| Tipo de datos | Reemplazo nulo |
|-----------------|-----------------------------------------------------|
| Num茅rico | Los valores nulos se sustituyen por el valor medio de la columna. |
| Categ贸rico | Los valores nulos se reemplazan con la palabra clave `ml_unknown`. |
| Booleano | Los valores nulos se reemplazan con un valor `FALSE`. |
| Marca de tiempo | Se espera que este sea un campo continuo. |
| Anidado/STRUCT | El reemplazo depende del tipo de datos del nodo de hoja. |

### Transformaci贸n de funciones {#automatic-feature-transformation}

| Tipo de datos | Transformaci贸n de funciones |
|-----------------|-----------------------------------------------------|
| Num茅rico | NO OBLIGATORIO: dado que los algoritmos de aprendizaje autom谩tico comprenden este tipo de datos. |
| Cadena | Se produce la indexaci贸n de cadenas. |
| Booleano | Se produce la indexaci贸n de cadenas. |
| Marca de tiempo | No se produce ninguna operaci贸n. |
| STRUCT | El valor se expande a su nodo de hoja. La transformaci贸n se produce en funci贸n del tipo de datos del nodo de hoja. |

**ejemplo**

```sql
CREATE model modelname options(model_type='logistic_reg', label='rating') AS SELECT * FROM movie_rating;
```

## Transformaciones manuales de funciones {#manual-transformations}

Para definir el preprocesamiento de datos personalizados en la instrucci贸n `CREATE MODEL`, utilice la cl谩usula `TRANSFORM` en combinaci贸n con cualquier n煤mero de funciones de transformaci贸n disponibles. Estas funciones de preprocesamiento manual tambi茅n se pueden utilizar fuera de la cl谩usula `TRANSFORM`. Todas las transformaciones que se discuten en la secci贸n [transformador a continuaci贸n](#available-transformations), se pueden usar para preprocesar los datos manualmente.

### Caracter铆sticas principales {#key-characteristics}

Las siguientes son caracter铆sticas clave de la transformaci贸n de funciones que se deben tener en cuenta al definir las funciones de preprocesamiento:

- **Sintaxis**: `TRANSFORM(functionName(colName, parameters) <aliasNAME>)`
   - El nombre del alias es obligatorio en la sintaxis. Debe proporcionar un nombre de alias o la consulta fallar谩.

- **Par谩metros**: los par谩metros son argumentos de posici贸n. Esto significa que cada par谩metro solo puede tomar determinados valores y requerir que se especifiquen todos los par谩metros anteriores si se proporcionan valores personalizados. Consulte la documentaci贸n relevante para obtener detalles sobre qu茅 funci贸n toma qu茅 argumento.

- **Transformadores de encadenamiento**: La salida de un transformador puede convertirse en la entrada de otro transformador.

- **Uso de caracter铆sticas**: la 煤ltima transformaci贸n de caracter铆sticas se usa como caracter铆stica del modelo de aprendizaje autom谩tico.

**Ejemplo**

```sql
CREATE MODEL modelname 
TRANSFORM(
  string_imputer(language, 'adding_null') AS imp_language, 
  numeric_imputer(users_count, 'mode') AS imp_users_count, 
  string_indexer(imp_language) AS si_lang,  
  vector_assembler(array(imp_users_count, si_lang, watch_minutes)) AS features
)  
OPTIONS(MODEL_TYPE='logistic_reg', LABEL='rating') 
AS SELECT * FROM df;
```

## Transformaciones disponibles {#available-transformations}

Hay 19 transformaciones disponibles. Estas transformaciones se dividen en [transformaciones generales](#general-transformations), [transformaciones num茅ricas](#numeric-transformations), [transformaciones categ贸ricas](#categorical-transformations) y [transformaciones textuales](#textual-transformations).

### Transformaciones generales {#general-transformations}

Lea esta secci贸n para obtener m谩s informaci贸n sobre los transformadores utilizados para una amplia gama de tipos de datos. Esta informaci贸n es esencial si necesita aplicar transformaciones que no sean espec铆ficas de los datos categ贸ricos o de texto.

>[!NOTE]
>
>El tipo de datos de entrada hace referencia a la columna en la que se aplica la imputaci贸n. El tipo de datos de salida hace referencia a la columna que se produce como salida despu茅s de que la transformaci贸n haya surtido efecto.

#### Imputador num茅rico {#numeric-imputer}

El transformador **Imputador num茅rico** completa los valores que faltan en un conjunto de datos. Utiliza la media, la mediana o el modo de las columnas en las que se encuentran los valores que faltan. Las columnas de entrada deben ser `DoubleType` o `FloatType`. Encontrar谩 m谩s informaci贸n y ejemplos en la [documentaci贸n del algoritmo Spark](https://spark.apache.org/docs/2.2.0/ml-features.html#imputer).

>[!NOTE]
>
>Todos los valores nulos de las columnas de entrada se tratan como ausentes y, por lo tanto, tambi茅n se imputan.

**Tipos de datos**

- Tipo de datos de entrada: num茅rico
- Tipo de datos de salida: num茅rico

**Definici贸n**

```sql
transformer(numeric_imputer(hour, 'mean') hour_imputed)
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
| -------- | ------------ | ----- | -------- | -------- |
| `STRATEGY` | Una estrategia de imputaci贸n. Los valores disponibles son: [`mean`, `median`, `mode`]. | cadena | malo | opcional |

{style="table-layout:auto"}

**Ejemplo antes de la imputaci贸n**

| Identificaci贸n | hora |
|---|---|
| 0 | 18,0 |
| 1 | null |
| 2 | 8,0 |

**Ejemplo despu茅s de la imputaci贸n (con estrategia media)**

| Identificaci贸n | hora |
|---|---|
| 0 | 18,0 |
| 1 | 13,0 |
| 2 | 8,0 |

#### Imputador de cadena {#string-imputer}

El transformador **String imputer** completa los valores que faltan en un conjunto de datos utilizando una cadena proporcionada por el usuario como argumento de funci贸n. Las columnas de entrada y salida deben ser del tipo de datos `string`.

>[!NOTE]
>
>Todos los valores nulos de las columnas de entrada se tratan como ausentes y se sustituyen por la cadena especificada.

**Tipos de datos**

- Tipo de datos de entrada: cadena
- Tipo de datos de salida: cadena

**Definici贸n**

```sql
transform(string_imputer(name, 'unknown_name') as name_imputed)
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
| -------- | ------------ | ----- | -------- | -------- |
| `NULL_REPLACEMENT` | El valor que reemplaza los valores nulos. | cadena | ml_unknown | opcional |

{style="table-layout:auto"}

**Ejemplo antes de la imputaci贸n**

| Identificaci贸n | name |
|---|---|
| 0 | John |
| 1 | null |
| 2 | Alice |

**Ejemplo despu茅s de la imputaci贸n (utilizando &#39;ml_unknown&#39; como reemplazo)**

| Identificaci贸n | name |
|---|---|
| 0 | John |
| 1 | ml_unknown |
| 2 | Alice |

#### Imputador booleano {#boolean-imputer}

El transformador **Boolean imputer** completa los valores que faltan en un conjunto de datos para una columna booleana. Las columnas de entrada y salida deben ser del tipo `Boolean`.

>[!NOTE]
>
>Todos los valores nulos de las columnas de entrada se tratan como ausentes y se sustituyen por el valor booleano especificado.

**Tipos de datos**

- Tipo de datos de entrada: booleano
- Tipo de datos de salida: booleano

**Definici贸n**

```sql
transform(boolean_imputer(name, true) as name_imputed)
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
| -------- | ------------ | ----- | -------- | -------- |
| `NULL_REPLACEMENT` | Imputador booleano. Valores permitidos: [`true`, `false`]. | Booleano | false | opcional |

**Ejemplo antes de la imputaci贸n**

| Identificaci贸n | indicador |
|---|---|
| 0 | true |
| 1 | null |
| 2 | false |

**Ejemplo despu茅s de la imputaci贸n (se usa &#39;true&#39; como reemplazo)**

| Identificaci贸n | indicador |
|---|---|
| 0 | true |
| 1 | true |
| 2 | false |

#### Ensamblador vectorial {#vector-assembler}

El transformador `VectorAssembler` combina una lista especificada de columnas de entrada en una sola columna vectorial, lo que facilita la administraci贸n de varias funciones en los modelos de aprendizaje autom谩tico. Esto resulta particularmente 煤til para combinar funciones sin procesar y aquellas generadas por diferentes transformadores de funciones en un vector de funciones unificado. `VectorAssembler` acepta columnas de entrada de tipos num茅ricos, booleanos y vectoriales. En cada fila, los valores de las columnas de entrada se concatenan en un vector en el orden especificado.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#vectorassembler) -->

**Tipos de datos**

- Tipo de datos de entrada: `array[string]` (nombres de columna con valores num茅ricos/array[numeric])
- Tipo de datos de salida: `Vector[double]`

**Definici贸n**

```sql
transform(vector_assembler(id, hour, mobile, userFeatures) as features)
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
| -------- | ------------ | ----- | -------- | -------- |
| NA | No se requieren par谩metros adicionales para este transformador. | NA | NA | NA |

{style="table-layout:auto"}

**Ejemplo antes de la transformaci贸n**

| Identificaci贸n | hora | m贸vil | userFeatures | pulsado |
|---|-------|--------|------------------|---------|
| 0 | 18 | 1,0 | [0,0; 10,0; 0,5] | 1,0 |

{style="table-layout:auto"}

**Ejemplo despu茅s de la transformaci贸n**

| Identificaci贸n | hora | m贸vil | userFeatures | pulsado | caracter铆sticas |
|---|------|--------|------------------|---------|-------------------------------|
| 0 | 18 | 1,0 | [0,0; 10,0; 0,5] | 1,0 | [18.0, 1.0, 0.0, 10.0, 0.5] |

{style="table-layout:auto"}

### Transformaciones num茅ricas {#numeric-transformations}

Lea esta secci贸n para obtener m谩s informaci贸n sobre los transformadores disponibles para procesar y escalar datos num茅ricos. Estos transformadores son necesarios para gestionar y optimizar las funciones num茅ricas de los conjuntos de datos.

#### Binarizador {#binarizer}

El transformador `Binarizer` convierte las caracter铆sticas num茅ricas en caracter铆sticas binarias (0/1) mediante un proceso denominado binarizaci贸n. Los valores mayores que el umbral especificado se convierten en 1,0, mientras que los valores iguales o menores que el umbral se convierten en 0,0. `Binarizer` admite los tipos `Vector` y `Double` para la columna de entrada.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#binarizer). -->

**Tipos de datos**

- Tipo de datos de entrada: columna num茅rica
- Tipo de datos de salida: num茅rico

**Definici贸n**

```sql
transform(numeric_imputer(rating, 'mode') rating_imp, binarizer(rating_imp) rating_binarizer)
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
|------------|----------------------------------------------------------------------------------------------------------|----------|----------|----------|
| `THRESHOLD` | Par谩metro para el umbral utilizado para binarizar funciones continuas. Las caracter铆sticas superiores al umbral se binarizan en 1,0, mientras que las caracter铆sticas iguales o inferiores al umbral se binarizan en 0,0. | int/double | 0,0 | opcional |

{style="table-layout:auto"}

**Entrada de ejemplo antes de la binarizaci贸n**

| Identificaci贸n | clasificaci贸n |
|---|---------|
| 0 | -18,0 |
| 1 | 13,0 |
| 2 | 8,0 |

**Ejemplo de salida despu茅s de la binarizaci贸n (umbral predeterminado de 0,0)**

| Identificaci贸n | clasificaci贸n |
|---|---------|
| 0 | 0,0 |
| 1 | 1,0 |
| 2 | 1,0 |

**Definici贸n con umbral personalizado**

```sql
transform(numeric_imputer(age, 'mode') age_imp, binarizer(age_imp, 14.0) age_binarizer)
```

**Ejemplo de salida despu茅s de la binarizaci贸n (con un umbral de 14.0)**

| Identificaci贸n | edad |
|---|-------|
| 0 | 0,0 |
| 1 | 0,0 |
| 2 | 1,0 |

#### Bucketizer {#bucketizer}

El transformador `Bucketizer` convierte una columna de caracter铆sticas continuas en una columna de contenedores de caracter铆sticas, seg煤n los umbrales especificados por el usuario. Este proceso es 煤til para segmentar datos continuos en grupos o bloques discretos. El `Bucketizer` requiere un par谩metro `splits`, que define los l铆mites de los contenedores.

**Tipos de datos**

- Tipo de datos de entrada: columna num茅rica
- Tipo de datos de salida: num茅rico (valores agrupados)

**Definici贸n**

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling, min_max_scaler(maxScaling) as features)
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
|----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|----------|----------|
| `splits` | Un par谩metro para asignar funciones continuas a contenedores. Con `n+1` divisiones, hay `n` bloques. Las divisiones deben estar en orden estrictamente creciente y se utiliza el rango (x,y) para cada bloque excepto el 煤ltimo, que incluye y. | array(doble) | N/A | opcional |

{style="table-layout:auto"}

**Ejemplos de divisiones**

- Matriz(Double.NegativeInfinity, 0.0, 1.0, Double.PositiveInfinity)
- Matriz (0.0, 1.0, 2.0)

Las divisiones deben cubrir todo el rango de valores Double; de lo contrario, los valores fuera de las divisiones especificadas se tratar谩n como errores.

**Ejemplo de transformaci贸n**

Este ejemplo toma una columna de caracter铆sticas continuas (`course_duration`), la agrupa seg煤n el elemento proporcionado `splits` y, a continuaci贸n, ensambla los contenedores resultantes con otras caracter铆sticas.

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling, min_max_scaler(maxScaling) as features)
```

#### MinMaxScaler {#minmaxscaler}

El transformador `MinMaxScaler` vuelve a escalar cada caracter铆stica de un conjunto de datos de filas vectoriales a un intervalo especificado, normalmente [0, 1]. Esto garantiza que todas las funciones contribuyan de forma equitativa al modelo. Resulta particularmente 煤til para modelos sensibles al escalado de caracter铆sticas, como los algoritmos basados en degradado y descenso. `MinMaxScaler` funciona con los siguientes par谩metros:

- **min**: l铆mite inferior de la transformaci贸n, compartido por todas las caracter铆sticas. El valor predeterminado es `0.0`.
- **max**: el l铆mite superior de la transformaci贸n, compartido por todas las caracter铆sticas. El valor predeterminado es `1.0`.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#minmaxscaler).  -->

**Tipos de datos**

- Tipo de datos de entrada: `Array[Double]`
- Tipo de datos de salida: `Array[Double]`

**Definici贸n**

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling, min_max_scaler(maxScaling) as features)
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
|-----------|--------------------------------------------------------------------------------------------------|------|---------|----------|
| `min` | L铆mite inferior despu茅s de la transformaci贸n, compartido por todas las funciones. | doble | 0,0 | opcional |
| `max` | L铆mite superior despu茅s de la transformaci贸n, compartido por todas las funciones. | doble | 1,0 | opcional |

**Ejemplo de transformaci贸n**

En este ejemplo se transforma un conjunto de caracter铆sticas y se cambia su escala al rango especificado mediante MinMaxScaler despu茅s de aplicar otras transformaciones.

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling, min_max_scaler(maxScaling) as features)
```

#### MaxAbsScaler {#maxabsscaler}

El transformador `MaxAbsScaler` vuelve a escalar cada caracter铆stica de un conjunto de datos de filas vectoriales al rango [-1, 1] dividiendo por el valor absoluto m谩ximo de cada caracter铆stica. Esta transformaci贸n es ideal para preservar la dispersi贸n en conjuntos de datos con valores positivos y negativos, ya que no desplaza ni centra los datos. Esto hace que `MaxAbsScaler` sea especialmente adecuado para modelos que son sensibles a la escala de las caracter铆sticas de entrada, como aquellos que implican c谩lculos de distancia.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#maxabsscaler). -->

**Tipos de datos**

- Tipo de datos de entrada: `Array[Double]`
- Tipo de datos de salida: `Array[Double]`

**Definici贸n**

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling)
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
|-----------|---------------------------------------------------------------------------------------------------------|------|---------|----------|
| NA | MaxAbsScaler no requiere ning煤n par谩metro adicional para su funcionamiento. | NA | NA | NA |

**Ejemplo de transformaci贸n**

Este ejemplo aplica varias transformaciones, incluido `MaxAbsScaler`, para cambiar el tama帽o de las caracter铆sticas en el rango [-1, 1].

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling)
```

#### Normalizador {#normalizer}

El `Normalizer` es un transformador que normaliza cada vector en un conjunto de datos de filas vectoriales para que tenga una norma unitaria. Este proceso asegura una escala consistente sin alterar la direcci贸n de los vectores. Esta transformaci贸n es particularmente 煤til en los modelos de aprendizaje autom谩tico que dependen de medidas a distancia u otros c谩lculos basados en vectores, especialmente cuando la magnitud de los vectores var铆a significativamente.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#normalizer) -->

**Tipos de datos**

- Tipo de datos de entrada: `array[double]` / `vector[double]`
- Tipo de datos de salida: `vector[double]`

**Definici贸n**

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, normalizer(vec_assembler, 3) as normalized)
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
|-----------|----------------------------------------------------------------------------------------|---------|---------|----------|
| `p` | Especifica el `p-norm` utilizado para la normalizaci贸n (por ejemplo, `1-norm`, `2-norm`, etc.). | entero | 2 | opcional |

**Ejemplo de transformaci贸n**

En este ejemplo se muestra c贸mo aplicar varias transformaciones, incluida la `Normalizer`, para normalizar un conjunto de caracter铆sticas utilizando la `p-norm` especificada.

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, normalizer(vec_assembler, 3) as normalized)
```

#### CuantilDiscretizer {#quantilediscretizer}

`QuantileDiscretizer` es un transformador que convierte una columna con caracter铆sticas continuas en caracter铆sticas categ贸ricas agrupadas, con el n煤mero de grupos determinado por el par谩metro `numBuckets`. En algunos casos, el n煤mero real de contenedores puede ser menor que el n煤mero especificado si hay demasiado pocos valores distintos para crear suficientes cuantiles.

Esta transformaci贸n es especialmente 煤til para simplificar la representaci贸n de datos continuos o prepararlos para algoritmos que funcionen mejor con entradas categ贸ricas.

**Tipos de datos**

- Tipo de datos de entrada: columna num茅rica
- Tipo de datos de salida: columna num茅rica (categ贸rica)

**Definici贸n**

```sql
TRANSFORM(quantile_discretizer(hour, 3) as result)
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
|--------------|--------------------------------------------------------------------------------------------------------------------------|---------|---------|----------|
| `NUM_BUCKETS` | El n煤mero de bloques (cuantiles o categor铆as) en los que se agrupan los puntos de datos. Este n煤mero debe ser mayor o igual que dos. | entero | 2 | opcional |

**Ejemplo de transformaci贸n**

En este ejemplo se muestra c贸mo `QuantileDiscretizer` agrupa una columna de caracter铆sticas continuas (`hour`) en tres bloques categ贸ricos.

```sql
TRANSFORM(quantile_discretizer(hour, 3) as result)
```

**Ejemplo antes y despu茅s de la discretizaci贸n**

| Identificaci贸n | hora | resultado |
|---|------|--------|
| 0 | 18,0 | 2.0 |
| 1 | 19,0 | 2.0 |
| 2 | 8,0 | 1,0 |
| 3 | 5.0 | 1,0 |
| 4 | 2,2 | 0,0 |

#### Escalador est谩ndar {#standardscaler}

El `StandardScaler` es un transformador que normaliza cada caracter铆stica de un conjunto de datos de filas vectoriales para que tenga una desviaci贸n est谩ndar unitaria o una media cero. Este proceso hace que los datos sean m谩s adecuados para los algoritmos que suponen que las funciones est谩n centradas en torno a cero con una escala coherente. Esta transformaci贸n es particularmente importante para modelos de aprendizaje autom谩tico como la VM, la regresi贸n log铆stica y las redes neuronales, donde los datos no estandarizados podr铆an dar lugar a problemas de convergencia o a una menor precisi贸n.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#standardscaler).  -->

**Tipos de datos**

- Tipo de datos de entrada: Vector
- Tipo de datos de salida: Vector

**Definici贸n**

```sql
TRANSFORM(standard_scaler(feature) as ss_features)
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
|------------|------------------------------------------------------------------------------------------------------|---------|---------|----------|
| `withStd` | Escala los datos para que tengan una desviaci贸n est谩ndar unitaria. | Booleano | True | opcional |
| `withMean` | Centra los datos con la media antes de escalar. Esta opci贸n produce una salida densa, por lo que tenga cuidado con la entrada dispersa. | Booleano | False | opcional |

**Ejemplo de transformaci贸n**

En este ejemplo se muestra c贸mo aplicar StandardScaler a un conjunto de caracter铆sticas, normaliz谩ndolas con la desviaci贸n est谩ndar unitaria y la media cero.

```sql
TRANSFORM(standard_scaler(feature) as ss_features)
```

### Transformaciones categor铆as {#categorical-transformations}

Lea esta secci贸n para obtener una descripci贸n general de los transformadores disponibles dise帽ados para convertir y preprocesar datos categ贸ricos para modelos de aprendizaje autom谩tico. Estas transformaciones est谩n dise帽adas para puntos de datos que representan categor铆as o etiquetas distintas, en lugar de valores num茅ricos.

#### StringIndexer {#stringindexer}

`StringIndexer` es un transformador que codifica una columna de cadena de etiquetas en una columna de 铆ndices num茅ricos. Los 铆ndices var铆an de 0 a `numLabels` y se ordenan por frecuencia de etiqueta (la etiqueta m谩s frecuente recibe un 铆ndice de 0). Si la columna de entrada es num茅rica, se convierte en una cadena antes de la indexaci贸n. Las etiquetas que no se ven se pueden asignar al 铆ndice `numLabels` si lo especifica el usuario.

Esta transformaci贸n es particularmente 煤til para convertir datos de cadena categ贸ricos en forma num茅rica, lo que lo hace adecuado para modelos de aprendizaje autom谩tico que requieren entrada num茅rica.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#stringindexer) -->

**Tipos de datos**

- Tipo de datos de entrada: cadena
- Tipo de datos de salida: num茅rico

**Definici贸n**

```sql
TRANSFORM(string_indexer(category) as si_category)
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
|-----------|-------------|------|---------|----------|
| NA | `StringIndexer` no requiere ning煤n par谩metro adicional para su funcionamiento. | NA | NA | NA |

**Ejemplo de transformaci贸n**

En este ejemplo se muestra c贸mo aplicar `StringIndexer` a una caracter铆stica categ贸rica, convirti茅ndola en un 铆ndice num茅rico.

```sql
TRANSFORM(string_indexer(category) as si_category)
```

#### OneHotEncoder {#onehotencoder}

El `OneHotEncoder` es un transformador que convierte una columna de 铆ndices de etiquetas en una columna de vectores binarios dispersos, donde cada vector tiene como m谩ximo un solo valor. Esta codificaci贸n es especialmente 煤til para permitir que los algoritmos que requieren entrada num茅rica, como Regresi贸n log铆stica, incorporen datos categ贸ricos de forma eficaz.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#onehotencoder).  -->

**Tipos de datos**

- Tipo de datos de entrada: num茅rico
- Tipo de datos de salida: Vector[Int]

**Definici贸n**

```sql
TRANSFORM(string_indexer(category) as si_category, one_hot_encoder(si_category) as ohe_category)
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
|-----------|-------------|------|---------|----------|
| NA | OneHotEncoder no requiere ning煤n par谩metro adicional para su funcionamiento. | NA | NA | NA |

**Ejemplo de transformaci贸n**

Este ejemplo muestra c贸mo aplicar primero `StringIndexer` a una caracter铆stica categ贸rica y, a continuaci贸n, utilizar `OneHotEncoder` para convertir los valores indizados en un vector binario.

```sql
TRANSFORM(string_indexer(category) as si_category, one_hot_encoder(si_category) as ohe_category)
```

### Transformaciones textuales {#textual-transformations}

Esta secci贸n proporciona detalles sobre los transformadores disponibles para procesar y convertir datos de texto en formatos que los modelos de aprendizaje autom谩tico pueden utilizar. Esta secci贸n es crucial para los desarrolladores que trabajan con datos y an谩lisis de texto en lenguaje natural.

#### CountVectorizer {#countvectorizer}

`CountVectorizer` es un transformador que convierte una colecci贸n de documentos de texto en vectores de recuentos de tokens, produciendo representaciones dispersas basadas en el vocabulario extra铆do del corpus. Esta transformaci贸n es esencial para convertir los datos de texto en un formato num茅rico que se puede utilizar en algoritmos de aprendizaje autom谩tico, como LDA (Asignaci贸n de Dirichlet Latente), representando la frecuencia de tokens dentro de cada documento.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#countvectorizer). -->

**Tipos de datos**

- Tipo de datos de entrada: Matriz[Cadena]
- Tipo de datos de salida: Vector denso

**Definici贸n**

```sql
TRANSFORM(count_vectorizer(texts) as cv_output)
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
|-----------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|---------|----------|
| `VOCAB_SIZE` | Tama帽o m谩ximo del vocabulario. CountVectorizer crea un vocabulario que solo considera los t茅rminos principales `vocabSize` ordenados por frecuencia de t茅rminos en todo el corpus. | Int | 218 | opcional |
| `MIN_DOC_FREQ` | Especifica el n煤mero m铆nimo de documentos diferentes en los que debe aparecer un t茅rmino para que se incluya en el vocabulario. Puede ser un n煤mero absoluto o una fracci贸n de documentos (si es doble). | Duplicada | 1,0 | opcional |
| `MAX_DOC_FREQ` | Especifica el n煤mero m谩ximo de documentos diferentes en los que podr铆a aparecer un t茅rmino para incluirlos en el vocabulario. Puede ser un n煤mero absoluto o una fracci贸n de documentos (si es doble). | Duplicada | (263)-1 | opcional |
| `MIN_TERM_FREQ` | Filtra las palabras raras de un documento. Se omiten los t茅rminos con una frecuencia/recuento inferior al umbral dado. Puede ser un n煤mero absoluto o una fracci贸n del recuento de tokens del documento. | Duplicada | 1,0 | opcional |

{style="table-layout:auto"}

**Ejemplo de transformaci贸n**

En este ejemplo se muestra c贸mo CountVectorizer convierte una colecci贸n de matrices de texto en vectores de recuentos de s铆mbolos, lo que produce una representaci贸n dispersa.

```sql
TRANSFORM(count_vectorizer(texts) as cv_output)
```

**Ejemplo antes y despu茅s de la vectorizaci贸n**

| Identificaci贸n | textos | cv_output |
|----|---------------------------------|-----------------------------------|
| 0 | Array(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) | (3,[0,1,2],[1.0,1.0,1.0]) |
| 1 | Array(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;) | (3,[0,1,2],[2.0,2.0,1.0]) |

{style="table-layout:auto"}

#### NGram {#ngram}

El `NGram` es un transformador que genera una secuencia de n-gramos, donde un n-gramo es una secuencia de tokens (&#39;??&#39;) (normalmente palabras) para alg煤n entero (``). El resultado consiste en cadenas delimitadas por espacios de palabras consecutivas &quot;??&quot;, que pueden utilizarse como caracter铆sticas en modelos de aprendizaje autom谩tico, especialmente aquellos centrados en el procesamiento de lenguajes naturales.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#n-gram). -->

**Tipos de datos**

- Tipo de datos de entrada: Matriz[Cadena]
- Tipo de datos de salida: Array[String]

**Definici贸n**

```sql
TRANSFORM(tokenizer(review_comments) as token_comments, ngram(token_comments, 3) as n_tokens)
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
|-----------|-----------------------------------------------------------------------------------------------|---------|-------------------|----------|
| `N` | La longitud m铆nima de n gramos debe ser mayor o igual que 1. | entero | 2 (caracter铆sticas del bigrama) | opcional |

{style="table-layout:auto"}

**Ejemplo de transformaci贸n**

Este ejemplo muestra c贸mo el transformador NGram crea una secuencia de 3 gramos a partir de una lista de tokens derivados de datos de texto.

```sql
TRANSFORM(tokenizer(review_comments) as token_comments, ngram(token_comments, 3) as n_tokens)
```

**Ejemplo antes y despu茅s de la transformaci贸n de n-gramos**

| Identificaci贸n | textos | n_tokens |
|----|-------------------------------------------------------|-------------------------------------------------------|
| 0 | [&quot;esto&quot;, &quot;era&quot;, &quot;un&quot;, &quot;entretenido&quot;, &quot;pel铆cula&quot;] | [&quot;esto fue una&quot;, &quot;fue una entretenida&quot;, &quot;una entretenida pel铆cula&quot;] |

{style="table-layout:auto"}

#### StopWordsRemover {#stopwordsremover}

`StopWordsRemover` es un transformador que elimina las palabras de detenci贸n de una secuencia de cadenas, filtrando las palabras comunes que no tienen un significado significativo. Toma como entrada una secuencia de cadenas (como la salida de un token) y elimina todas las palabras de detenci贸n especificadas por el par谩metro `stopWords`.

Esta transformaci贸n es 煤til para el preprocesamiento de datos de texto, ya que mejora la eficacia de los modelos de aprendizaje autom谩tico posteriores al eliminar palabras que no contribuyen mucho al significado general.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#stopwordsremover) -->

**Tipos de datos**

- Tipo de datos de entrada: Matriz[Cadena]
- Tipo de datos de salida: Array[String]

**Definici贸n**

```sql
TRANSFORM(stop_words_remover(raw) as filtered)
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
|--------------------|--------------------------------------------------------------------------------------------------|---------------|-------------------------|----------|
| `stopWords` | Las palabras que se filtrar谩n. | matriz [string] | Predeterminado: palabras de detenci贸n en ingl茅s | opcional |

{style="table-layout:auto"}

<!-- Q) should this be the `CUSTOM_STOP_WORDS` parameter or the `stopWords` parameter?  -->

**Ejemplo de transformaci贸n**

Este ejemplo muestra c贸mo `StopWordsRemover` filtra las palabras de detenci贸n en ingl茅s comunes de una lista de tokens.

```sql
TRANSFORM(stop_words_remover(raw) as filtered)
```

**Ejemplo antes y despu茅s de detener la eliminaci贸n de palabras**

| Identificaci贸n | crudo | filtrado |
|----|------------------------------|--------------------------|
| 0 | [Yo, vi, el, rojo, globo] | [sierra, roja, globo] |
| 1 | [Mar铆a, ten铆a, un, peque帽o, cordero] | [Mar铆a, peque帽a, cordero] |

**Ejemplo con palabras de detenci贸n personalizadas**

En este ejemplo se muestra c贸mo utilizar una lista personalizada de palabras de detenci贸n para filtrar palabras espec铆ficas de las secuencias de entrada.

```sql
TRANSFORM(stop_words_remover(raw, array("red", "I", "had")) as filtered)
```

**Ejemplo antes y despu茅s de la eliminaci贸n de palabras de detenci贸n personalizadas**

| Identificaci贸n | crudo | filtrado |
|----|------------------------------|--------------------------|
| 0 | [Yo, vi, el, rojo, globo] | [sierra, el, globo] |
| 1 | [Mar铆a, ten铆a, un, peque帽o, cordero] | [Mar铆a, una, peque帽a, cordero] |

#### TF-IDF {#tf-idf}

`TF-IDF` (Frecuencia de t茅rmino-Frecuencia de documento inversa) es un transformador que se utiliza para medir la importancia de una palabra dentro de un documento en relaci贸n con un corpus. La frecuencia de t茅rmino (TF) se refiere al n煤mero de veces que aparece un t茅rmino \(t\) en un documento \(d\), mientras que la frecuencia de documento (DF) mide cu谩ntos documentos del cuerpo \(D\) contienen el t茅rmino \(t\). Este m茅todo se utiliza ampliamente en la miner铆a de texto para reducir la influencia de palabras comunes, como &quot;a&quot;, &quot;the&quot; y &quot;of&quot;, que contienen poca informaci贸n 煤nica.

Esta transformaci贸n es especialmente valiosa en las tareas de miner铆a de texto y procesamiento de lenguajes naturales, ya que asigna un valor num茅rico a la importancia de cada palabra dentro de un documento y en todo el corpus.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#tf-idf) -->

**Tipos de datos**

- Tipo de datos de entrada: Matriz[Cadena]
- Tipo de datos de salida: Vector[Int]

**Definici贸n**

```sql
create table td_idf_model transform(tokenizer(sentence) as token_sentence, tf_idf(token_sentence) as tf_sentence, vector_assembler(array(tf_sentence)) as feature) OPTIONS()
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
|-----------------|----------------------------------------------------------------------------------------|------|---------|----------|
| `NUM_FEATURES` | N煤mero de funciones que se van a generar. Debe ser mayor que 0. | Int | 262144 | opcional |
| `MIN_DOC_FREQ` | El n煤mero m铆nimo de documentos en los que debe aparecer un t茅rmino para que se incluya en el modelo. | Int | 0 | opcional |

{style="table-layout:auto"}

**Ejemplo de transformaci贸n**

En este ejemplo se muestra c贸mo utilizar TF-IDF para transformar frases con s铆mbolo de token en un vector de caracter铆sticas que representa la importancia de cada t茅rmino en el contexto de todo el cuerpo.

```sql
create table td_idf_model transform(tokenizer(sentence) as token_sentence, tf_idf(token_sentence) as tf_sentence, vector_assembler(array(tf_sentence)) as feature) OPTIONS()
```

#### Tokenizador {#tokenizer}

`Tokenizer` es un transformador que desglosa texto, como una oraci贸n, en t茅rminos individuales, normalmente palabras. Convierte frases en matrices de tokens, proporcionando un paso fundamental en el preprocesamiento de texto que prepara los datos para un an谩lisis de texto o procesos de modelado adicionales.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#tokenizer) -->

**Tipos de datos**

- Tipo de datos de entrada: frase textual
- Tipo de datos de salida: Array[String]

**Definici贸n**

```sql
create table td_idf_model transform(tokenizer(sentence) as token_sentence, tf_idf(token_sentence) as tf_sentence, vector_assembler(array(tf_sentence)) as feature) OPTIONS()
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
|-----------|-------------|------|---------|----------|
| NA | `Tokenizer` no requiere ning煤n par谩metro adicional para su funcionamiento. | NA | NA | NA |

**Ejemplo de transformaci贸n**

En este ejemplo se muestra c贸mo `Tokenizer` desglosa las frases en palabras individuales (tokens) como parte de una canalizaci贸n de procesamiento de texto.

```sql
create table td_idf_model transform(tokenizer(sentence) as token_sentence, tf_idf(token_sentence) as tf_sentence, vector_assembler(array(tf_sentence)) as feature) OPTIONS()
```

#### Word2Vec {#word2vec}

`Word2Vec` es un estimador que procesa secuencias de palabras que representan documentos y entrena a `Word2VecModel`. Este modelo asigna cada palabra a un vector de tama帽o fijo 煤nico y transforma cada documento en un vector promediando los vectores de todas las palabras del documento. Ampliamente utilizado en tareas de procesamiento de lenguaje natural, `Word2Vec` crea incrustaciones de palabras que capturan el significado sem谩ntico, convirtiendo los datos de texto en vectores num茅ricos que representan las relaciones entre las palabras y permitiendo un an谩lisis de texto m谩s efectivo y modelos de aprendizaje autom谩tico.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#word2vec) -->

**Tipos de datos**

- Tipo de datos de entrada: Matriz[Cadena]
- Tipo de datos de salida: Vector[Double]

**Definici贸n**

```sql
TRANSFORM(tokenizer(review) as tokenized, word2Vec(tokenized, 10, 1) as word2Vec)
```

**Par谩metros**

| Par谩metro | Descripci贸n | Tipo | Predeterminado | Opcional |
|--------------|-----------------------------------------------------------------------------------------------------|---------|---------|----------|
| `VECTOR_SIZE` | Dimensi贸n del vector en el que se transforma cada palabra. | Entero | 100 | opcional |
| `MIN_COUNT` | El n煤mero m铆nimo de veces que debe aparecer un token para incluirse en el vocabulario del modelo `Word2Vec`. | Entero | 5 | opcional |

{style="table-layout:auto"}

**Ejemplo de transformaci贸n**

Este ejemplo muestra c贸mo `Word2Vec` convierte una revisi贸n con token en un vector de tama帽o fijo que representa el promedio de los vectores de palabras del documento.

```sql
TRANSFORM(tokenizer(review) as tokenized, word2Vec(tokenized, 10, 1) as word2Vec)
```

**Ejemplo antes y despu茅s de la transformaci贸n de Word2Vec**

| rese帽a | tokenizado | word2Vec |
|-------------------------------|--------------------------------------|---------------------------------|
| esta fue una pel铆cula entretenida | [esto, fue, una, entretenida, pel铆cula] | [-0.025713888928294182,0.00818799751577899,0.0092235435731709,-0.01515385233797133,0.012175946310162545,3.1129065901041035E-4,0.0025145105042611252,0.005757019785232843,-0.021328244300093502,0.009335877187550069] |

{style="table-layout:auto"}
